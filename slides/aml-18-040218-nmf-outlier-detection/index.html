<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre, .remark-slide-content .compact .MathJax_Display{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-content .smaller p, .remark-slide-content .smaller li,
      .remark-slide-content .smaller .remark-code, .remark-slide-content .smaller a{
          font-size: 25px;
      }
      .normal {
          font-size: 30px;
      }
      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# NMF; Outlier detection

04/02/18

Andreas C. Müller
???
FIXME matrix factorization should be approximately equal, not equal
FIXME list KL divergence loss for NMF
FIXME related gaussian model and Kernel density to GMM?
---
class: center, middle

# Non-Negative Matrix Factorization
???
---
#Matrix Factorization

<br>

.center[
![:scale 90%](images/matrix_factorization.png)
]
???
---
#Matrix Factorization

<br>

.center[
![:scale 90%](images/matrix_factorization_2.png)
]

???
---
#PCA

<br>

.center[
![:scale 80%](images/pca.png)
]
???
---
class:spacious
# Other Matrix Factorizations
    
- PCA: principal components orthogonal, minimize
squared loss

- Sparse PCA: components orthogonal and sparse

- ICA: independent components

- Non-negative matrix factorization (NMF):
latent representation and latent features are nonnegative.

???
---
#NMF

<br>

.center[
![:scale 80%](images/nmf.png)
]

???
---
class: spacious

# Why NMF?

--

- Meaningful signs

--

- Positive weights

--

- No “cancellation” like in PCA

--

- Can learn over-complete representation

???
- positive weights -> parts / components
- Can be viewed as “soft clustering”: each point is
positive linear combination of weights.

- (n_components > n_features) by asking for sparsity (in either W or H)

---
.center[
PCA (ordered by projection, not eigenvalue)

![:scale 70%](images/pca_projection.png)
]

.center[
NMF (ordered by hidden representation)
![:scale 70%](images/nmf_hid_rep.png)
]

???
---
# Downsides of NMF

- Can only be applied to non-negative data

--

- Interpretability is hit or miss

--

- Non-convex optimization, requires initialization

--

- Not orthogonal

--

.center[
![:scale 60%](images/nmf_downsides.png)
]
???
optimization for "projection"
---

.center[
NMF with 20 components on MNIST

![:scale 60%](images/nmf_20_mnist.png)
]

.center[
NMF with 5 components on MNIST

![:scale 60%](images/nmf_5_mnist.png)
]

???
---
class:spacious
# Applications of NMF

- Text analysis (next week)

- Signal processing

- Speech and Audio (see <a href="https://librosa.github.io/librosa/generated/librosa.decompose.decompose.html#librosa.decompose.decompose">librosa</a>)


- Source separation

- Gene expression analysis


???
---
class:center,middle
#Outlier Detection

???
---
#Motivation
.padding-top[
.left-column[
![:scale 100%](images/outlier_detection.png)
]

.right-column[
![:scale 100%](images/novelty_detection.png)
]
]
???

- Find points that are “different” within the training set (and in the future).

- “Novelty detection” - no outliers in the training set.

- Outliers are not labeled! (otherwise it’s just imbalanced classification)

- Often used interchangeably in practice.

---
class:spacious
# Applications

- Fraud detection (credit cards, click fraud, ...)

- Network failure detection

- Intrusion detection in networks

- Defect detection (engineering etc…)

- News? Intelligence?

???
---
class:spacious
# Basic idea

- Model data distribution $p(X)$

--

- Outlier: $p(X) < \varepsilon$

--

- For outlier detection: be robust in modelling $p(X)$

???

- Task is generally ill-defined (unless you know the
real data distribution).
- Task is generally ill-defined (unless you know the
real data distribution).
---
#Elliptic Envelope

`$$p(X) = \mathcal{N}(\mu, \Sigma)$$`

.center[
![:scale 60%](images/elliptic_envelope.png)
]

???
Fit robust covariance matrix and mean
FIXME add slide on Covariance: Minimum Covariance Determination (MinCovDet)

---
#Elliptic Envelope

- Preprocessing with PCA might help sometimes.

.smaller[
```python
from sklearn.covariance import EllipticEnvelope
ee = EllipticEnvelope(contamination=.1).fit(X)
pred = ee.predict(X)
print(pred)
print(np.mean(pred == -1))
```]

.smaller[
```python
[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
 -1  1  1  1 -1 -1  1 -1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1 -1  1 -1  1  1]
0.104

```]

???
---
# Failure-case: Non-Gaussian Data

.center[
![:scale 80%](images/elliptic_envelope_plot.png)
]

???
Could do mixtures of gaussians obviously!
---
class: center, spacious
#Kernel Density


![:scale 80%](images/kde_vs_histogram.png)

???
- Non-parametric density model

- Gaussian blob on each data point

- Doesn’t work well in high dimensions
---
class:center

# Kernel Bandwidth

![:scale 50%](images/kde_bandwidth.png)

???
- Need to adjust kernel bandwidth

- Unsupervised model, so how to pick kernel bandwidth?
- cross-validation can be used to pick the bandwidth, but if there's outliers in the training
data, could go wrong?


---

.smaller[
```python
kde = KernelDensity(bandwidth=3)
kde.fit(X_train_noise)
pred = kde.score_samples(X_train_noise)
pred = (pred > np.percentile(pred, 10)).astype(int)
```]

.center[
![:scale 70%](images/kernel_density_bw3.png)
]
???
---

# One Class SVM

- Also uses Gaussian kernel to cover data

- Only select support vectors (not all points)

- Specify outlier ratio (contamination) via nu


.smaller[
```python
from sklearn.svm import OneClassSVM
scaler = StandardScaler()
X_train_noise_scaled = scaler.fit_transform(X_train_noise)
oneclass = OneClassSVM(nu=.1).fit(X_train_noise_scaled)
pred = oneclass.predict(X_train_noise_scaled)
```]

???

- Need to adjust kernel bandwidth
- nu is "training mistakes"

---
.center[
![:scale 80%](images/one_class_svm_plot.png)
]

???
---
class:center,middle

#Isolation Forests

???
---
#Idea

- Outliers are easy to isolate from the rest

.center[
![:scale 80%](images/isolation_forests.png)
]


???
- Measure as Path length!

---
class:center,middle
.left-column[
![:scale 100%](images/isolation_forests.png)
]

.right-column[
![:scale 100%](images/avgpathlen_numtrees.png)
]
???
---
#Normalizing the Path Length

Average path length of unsuccessful search in Binary Search Tree:

`$$ c(n) = 2H(n-1) - \left(\frac{2(n-1)}{n}\right) \text{  (H = Harmonic number)}$$` 



`$$ s(x,n) = 2^{-\frac{E(h(x))}{c(n)}} \text{   (h = depth in tree)}$$` 


- s &lt; 0.5 : definite inlier

- s close to 1: outlier

???
---
class:spacious
# Building the forest

- Subsample dataset for each tree

- Default sample size of 256 works surprisingly well

- Stop growing tree at depth log_2(sample size) – so 8

- No bootstrapping

- More trees are better – default 100

- Need to specify contamination rate

???
FIXME all the text
---
.center[![:scale 90%](images/building_forest_1.png)]
???
---
.center[![:scale 90%](images/building_forest_2.png)]
???
---
class:spacious
# Other density based-models

- PCA

- GMMs

- Robust PCA (not in sklearn :-()

- Any other probabilistic model - “robust” is better.

???
---
.center[
![:scale 60%](images/other_density_models_1.png)
]

.center[
![:scale 60%](images/other_density_models_2.png)
]

.center[
![:scale 60%](images/other_density_models_3.png)
]
???
---
class:spacious
# Summary

- Isolation Forest works great!

- Density models are great if they are correct.

- Estimating bandwidth can be tricky in the
unsupervised setting.

- Validation of results often requires manual inspection.

???
---
class: middle
# Questions ?


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
